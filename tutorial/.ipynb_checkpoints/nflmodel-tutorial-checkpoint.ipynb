{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecasting NFL point spreads and point totals using the margin-dependent Elo model\n",
    "\n",
    "#### J. Scott Moreland | Philadelphia, PA | January 11th, 2020\n",
    "\n",
    "_A brief tutorial of an NFL implementation of the margin-dependent Elo (MELO) model._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro\n",
    "\n",
    "This Jupyter notebook describes the `nflmodel` Python package which can be used to predict the full probability distribution of NFL point-spread and point-total outcomes.\n",
    "\n",
    "The model is inspired by the Elo based sports analytics work at [fivethirtyeight.com](https://fivethirtyeight.com/) and makes use of a machine learning algorithm that I developed called the [margin-dependent Elo (MELO)](https://github.com/morelandjs/melo) model.\n",
    "\n",
    "I describe the theory behind the algorithm at length in an [arXiv pre-print](https://arxiv.org/abs/1802.00527), and you can also read about it on the [MELO sphinx documentation](https://moreland.dev/projects/melo/) page. The purpose of the blog post is not to rehash the theory behind the model, but rather to demonstrate how it can be used effectively in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "The `nflmodel` package requires Python3 and is intended to be used through a command line interface. I've tested the package on Arch Linux and OSX, but not on Windows.\n",
    "\n",
    "The model also requires an active internet connection to pull in the latest game data and schedule information. I'm working to enable offline capabilities, but this does not exist at the moment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "Navigate to the parent directory where you want to save the `nflmodel` package source, then run the following from the command line to install."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'nflmodel'...\n",
      "remote: Enumerating objects: 230, done.\u001b[K\n",
      "remote: Counting objects: 100% (230/230), done.\u001b[K\n",
      "remote: Compressing objects: 100% (141/141), done.\u001b[K\n",
      "remote: Total 347 (delta 117), reused 172 (delta 69), pack-reused 117\u001b[K\n",
      "Receiving objects: 100% (347/347), 286.46 KiB | 236.00 KiB/s, done.\n",
      "Resolving deltas: 100% (164/164), done.\n",
      "Processing ./nflmodel\n",
      "Requirement already satisfied: hyperopt in /home/morelandjs/.local/lib/python3.8/site-packages (from nflmodel==0.1) (0.2.2)\n",
      "Requirement already satisfied: matplotlib in /home/morelandjs/.local/lib/python3.8/site-packages (from nflmodel==0.1) (3.1.2)\n",
      "Requirement already satisfied: melo@ git+https://git@github.com/morelandjs/melo.git@dev#egg=melo from git+https://****@github.com/morelandjs/melo.git@dev#egg=melo in /home/morelandjs/.local/lib/python3.8/site-packages (from nflmodel==0.1) (1.1.0)\n",
      "Requirement already satisfied: nflgame_redux@ git+https://git@github.com/morelandjs/nflgame.git@master#egg=nflgame_redux from git+https://****@github.com/morelandjs/nflgame.git@master#egg=nflgame_redux in /home/morelandjs/.local/lib/python3.8/site-packages (from nflmodel==0.1) (2.0.1b1)\n",
      "Requirement already satisfied: numpy in /usr/lib/python3.8/site-packages (from nflmodel==0.1) (1.18.1)\n",
      "Requirement already satisfied: pandas in /home/morelandjs/.local/lib/python3.8/site-packages (from nflmodel==0.1) (0.25.3)\n",
      "Requirement already satisfied: scipy>=0.18.0 in /usr/lib/python3.8/site-packages (from nflmodel==0.1) (1.4.1)\n",
      "Requirement already satisfied: setuptools in /home/morelandjs/.local/lib/python3.8/site-packages (from nflmodel==0.1) (44.0.0)\n",
      "Requirement already satisfied: SQLAlchemy in /home/morelandjs/.local/lib/python3.8/site-packages (from nflmodel==0.1) (1.3.11)\n",
      "Requirement already satisfied: networkx==2.2 in /home/morelandjs/.local/lib/python3.8/site-packages (from hyperopt->nflmodel==0.1) (2.2)\n",
      "Requirement already satisfied: tqdm in /usr/lib/python3.8/site-packages (from hyperopt->nflmodel==0.1) (4.28.1)\n",
      "Requirement already satisfied: future in /usr/lib/python3.8/site-packages (from hyperopt->nflmodel==0.1) (0.18.2)\n",
      "Requirement already satisfied: six in /usr/lib/python3.8/site-packages (from hyperopt->nflmodel==0.1) (1.14.0)\n",
      "Requirement already satisfied: cloudpickle in /home/morelandjs/.local/lib/python3.8/site-packages (from hyperopt->nflmodel==0.1) (1.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/lib/python3.8/site-packages (from matplotlib->nflmodel==0.1) (2.8.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/lib/python3.8/site-packages (from matplotlib->nflmodel==0.1) (2.4.6)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/lib/python3.8/site-packages (from matplotlib->nflmodel==0.1) (1.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/lib/python3.8/site-packages (from matplotlib->nflmodel==0.1) (0.10.0)\n",
      "Requirement already satisfied: pytz in /usr/lib/python3.8/site-packages (from nflgame_redux@ git+https://git@github.com/morelandjs/nflgame.git@master#egg=nflgame_redux->nflmodel==0.1) (2019.3)\n",
      "Requirement already satisfied: lxml in /home/morelandjs/.local/lib/python3.8/site-packages (from nflgame_redux@ git+https://git@github.com/morelandjs/nflgame.git@master#egg=nflgame_redux->nflmodel==0.1) (4.4.1)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/morelandjs/.local/lib/python3.8/site-packages (from nflgame_redux@ git+https://git@github.com/morelandjs/nflgame.git@master#egg=nflgame_redux->nflmodel==0.1) (4.8.1)\n",
      "Requirement already satisfied: requests in /usr/lib/python3.8/site-packages (from nflgame_redux@ git+https://git@github.com/morelandjs/nflgame.git@master#egg=nflgame_redux->nflmodel==0.1) (2.22.0)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /usr/lib/python3.8/site-packages (from networkx==2.2->hyperopt->nflmodel==0.1) (4.4.1)\n",
      "Requirement already satisfied: soupsieve>=1.2 in /home/morelandjs/.local/lib/python3.8/site-packages (from beautifulsoup4->nflgame_redux@ git+https://git@github.com/morelandjs/nflgame.git@master#egg=nflgame_redux->nflmodel==0.1) (1.9.5)\n",
      "Requirement already satisfied: chardet>=3.0.2 in /usr/lib/python3.8/site-packages (from requests->nflgame_redux@ git+https://git@github.com/morelandjs/nflgame.git@master#egg=nflgame_redux->nflmodel==0.1) (3.0.4)\n",
      "Requirement already satisfied: idna>=2.5 in /usr/lib/python3.8/site-packages (from requests->nflgame_redux@ git+https://git@github.com/morelandjs/nflgame.git@master#egg=nflgame_redux->nflmodel==0.1) (2.8)\n",
      "Requirement already satisfied: urllib3>=1.21.1 in /usr/lib/python3.8/site-packages (from requests->nflgame_redux@ git+https://git@github.com/morelandjs/nflgame.git@master#egg=nflgame_redux->nflmodel==0.1) (1.25.8)\n",
      "Building wheels for collected packages: nflmodel\n",
      "  Building wheel for nflmodel (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for nflmodel: filename=nflmodel-0.1-cp38-none-any.whl size=11931 sha256=f33b8b008c864b0b6e2674da094fdc0cfc503e25514e2876157790bdbf7753ba\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-6tmp_pmn/wheels/21/ef/79/3b16cfd93d653441a0e3e3e028e984fc1843b1d10688fe9fbe\n",
      "Successfully built nflmodel\n",
      "Installing collected packages: nflmodel\n",
      "  Found existing installation: nflmodel 0.1\n",
      "    Uninstalling nflmodel-0.1:\n",
      "      Successfully uninstalled nflmodel-0.1\n",
      "Successfully installed nflmodel-0.1\n",
      "\u001b[33mWARNING: You are using pip version 19.3.1; however, version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/morelandjs/nfl-model.git nflmodel\n",
    "!pip3 install --user nflmodel/."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After installing the `nflmodel` package, you'll need to populate the database of NFL game data. Since this is presumably your first time running the package, it will download all available games dating back to the 2009 season."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO][data] updating season 2019 week 17\r\n"
     ]
    }
   ],
   "source": [
    "!nflmodel update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subsequent calls to `nflmodel update` will incrementally refresh the database and pull in all new game data since the last update. If at any point your database becomes corrupted, you can rebuild it from scratch using the optional `--rebuild` flag.\n",
    "\n",
    "Now that we've populated the database, let's inspect some of the game data contained within."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                datetime  season  week  ... team_away       qb_away  score_away\r\n",
      "0    2009-09-10 20:30:00    2009     1  ...       TEN     K.Collins          10\r\n",
      "1    2009-09-13 13:00:00    2009     1  ...       MIA  C.Pennington           7\r\n",
      "2    2009-09-13 13:00:00    2009     1  ...        KC      B.Croyle          24\r\n",
      "3    2009-09-13 13:00:00    2009     1  ...       PHI      D.McNabb          38\r\n",
      "4    2009-09-13 13:00:00    2009     1  ...       DEN       K.Orton          12\r\n",
      "...                  ...     ...   ...  ...       ...           ...         ...\r\n",
      "2811 2019-12-29 13:00:00    2019    17  ...       PHI       C.Wentz          34\r\n",
      "2812 2019-12-29 13:00:00    2019    17  ...       ATL        M.Ryan          28\r\n",
      "2813 2019-12-29 16:25:00    2019    17  ...       OAK        D.Carr          15\r\n",
      "2814 2019-12-29 16:25:00    2019    17  ...       ARI      K.Murray          24\r\n",
      "2815 2019-12-29 16:25:00    2019    17  ...        SF   J.Garoppolo          26\r\n",
      "\r\n",
      "[2816 rows x 9 columns]\r\n"
     ]
    }
   ],
   "source": [
    "!nflmodel games"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row corresponds to a single game, and each column is a certain attribute of that game. At the moment I populate the following attributes:\n",
    "\n",
    "* **datetime** - (datetime64) - game start time\n",
    "* **season** (int) - year in which the season started\n",
    "* **week** (int) - nfl season week 1–17\n",
    "* **team_home** (string) - home team city abbreviation\n",
    "* **qb_home** (string) - home team quarterback name, first initial dot surname\n",
    "* **score_home** (int) - home team points scored\n",
    "* **team_away** (string) - away team city abbreviation\n",
    "* **qb_away** (string) - away team quarterback name, first initial dot surname\n",
    "* **score_away** (int) - away team points scored"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, the `nflmodel games` runner is printing a pandas dataframe, so it may hide columns in order to fit the width of your terminal window. If you want to see more column output, try enlarging your terminal window.\n",
    "\n",
    "If you want to see more _row_ output, use the `--head` and `--tail` commands. For example,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             datetime  season  week  ... team_away       qb_away  score_away\r\n",
      "0 2009-09-10 20:30:00    2009     1  ...       TEN     K.Collins          10\r\n",
      "1 2009-09-13 13:00:00    2009     1  ...       MIA  C.Pennington           7\r\n",
      "2 2009-09-13 13:00:00    2009     1  ...        KC      B.Croyle          24\r\n",
      "\r\n",
      "[3 rows x 9 columns]\r\n"
     ]
    }
   ],
   "source": [
    "!nflmodel games --head 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The games dataframe is sorted from oldest to most recent, so `--head N` returns the N oldest games in the database and `--tail N` returns the N most recent games. If you want to print the entire dataset to standard out, just set `--head` or `--tail` to a very large number.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calibration\n",
    "\n",
    "The model predictions depend on a handful of hyperparameter values that are unknown _a priori_. These hyperparameters are:\n",
    "* **kfactor** (float) - prefactor multiplying the Elo rating update; a larger kfactor makes the ratings more reponsive to game outcomes\n",
    "* **regress_coeff** (float) - fraction to regress each rating to the mean after 3 months inactivity\n",
    "* **rest_bonus** (float) - prefactor multiplying each matchup's rest difference (or sum)\n",
    "* **exp_bonus** (float) - prefactor multiplying each matchup's quarterback experience difference (or sum)\n",
    "* **weight_qb** (float) - coeficient used to blend team ratings and quarterback ratings.\n",
    "\n",
    "Home field advantage is accounted for naturally by the `MELO` model constructor, so it is not included in the above list.\n",
    "\n",
    "The hyperparameter values are calibrated by the following command (this will take several minutes or so, so grab a drink)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO][model] calibrating spread hyperparameters\n",
      "[INFO][data] updating season 2019 week 17\n",
      "[INFO][tpe] tpe_transform took 0.001760 seconds\n",
      "[INFO][tpe] TPE using 0 trials\n",
      "[INFO][tpe] tpe_transform took 0.001830 seconds\n",
      "[INFO][tpe] TPE using 1/1 trials with best loss 10.418035\n",
      "[INFO][tpe] tpe_transform took 0.001730 seconds\n",
      "[INFO][tpe] TPE using 2/2 trials with best loss 10.418035\n",
      "[INFO][tpe] tpe_transform took 0.001709 seconds\n",
      "[INFO][tpe] TPE using 3/3 trials with best loss 10.418035\n",
      "[INFO][tpe] tpe_transform took 0.001820 seconds\n",
      "[INFO][tpe] TPE using 4/4 trials with best loss 10.418035\n",
      "[INFO][tpe] tpe_transform took 0.001699 seconds\n",
      "[INFO][tpe] TPE using 5/5 trials with best loss 10.401775\n",
      "[INFO][tpe] tpe_transform took 0.001771 seconds\n",
      "[INFO][tpe] TPE using 6/6 trials with best loss 10.401775\n",
      "[INFO][tpe] tpe_transform took 0.001753 seconds\n",
      "[INFO][tpe] TPE using 7/7 trials with best loss 10.395261\n",
      "[INFO][tpe] tpe_transform took 0.001766 seconds\n",
      "[INFO][tpe] TPE using 8/8 trials with best loss 10.395261\n",
      "[INFO][tpe] tpe_transform took 0.001958 seconds\n",
      "[INFO][tpe] TPE using 9/9 trials with best loss 10.395261\n",
      "[INFO][tpe] tpe_transform took 0.001766 seconds\n",
      "[INFO][tpe] TPE using 10/10 trials with best loss 10.395261\n",
      "[INFO][tpe] tpe_transform took 0.001703 seconds\n",
      "[INFO][tpe] TPE using 11/11 trials with best loss 10.395261\n",
      "[INFO][tpe] tpe_transform took 0.001772 seconds\n",
      "[INFO][tpe] TPE using 12/12 trials with best loss 10.395261\n",
      "[INFO][tpe] tpe_transform took 0.001738 seconds\n",
      "[INFO][tpe] TPE using 13/13 trials with best loss 10.395261\n",
      "[INFO][tpe] tpe_transform took 0.001789 seconds\n",
      "[INFO][tpe] TPE using 14/14 trials with best loss 10.389334\n",
      "[INFO][tpe] tpe_transform took 0.001757 seconds\n",
      "[INFO][tpe] TPE using 15/15 trials with best loss 10.385868\n",
      "[INFO][tpe] tpe_transform took 0.001723 seconds\n",
      "[INFO][tpe] TPE using 16/16 trials with best loss 10.385868\n",
      "[INFO][tpe] tpe_transform took 0.001752 seconds\n",
      "[INFO][tpe] TPE using 17/17 trials with best loss 10.385868\n",
      "[INFO][tpe] tpe_transform took 0.001765 seconds\n",
      "[INFO][tpe] TPE using 18/18 trials with best loss 10.385868\n",
      "[INFO][tpe] tpe_transform took 0.001771 seconds\n",
      "[INFO][tpe] TPE using 19/19 trials with best loss 10.385868\n",
      "[INFO][tpe] tpe_transform took 0.001913 seconds\n",
      "[INFO][tpe] TPE using 20/20 trials with best loss 10.385868\n",
      "[INFO][tpe] tpe_transform took 0.001727 seconds\n",
      "[INFO][tpe] TPE using 21/21 trials with best loss 10.385868\n",
      "[INFO][tpe] tpe_transform took 0.001682 seconds\n",
      "[INFO][tpe] TPE using 22/22 trials with best loss 10.385868\n",
      "[INFO][tpe] tpe_transform took 0.001806 seconds\n",
      "[INFO][tpe] TPE using 23/23 trials with best loss 10.385868\n",
      "[INFO][tpe] tpe_transform took 0.001743 seconds\n",
      "[INFO][tpe] TPE using 24/24 trials with best loss 10.385868\n",
      "[INFO][tpe] tpe_transform took 0.001755 seconds\n",
      "[INFO][tpe] TPE using 25/25 trials with best loss 10.382119\n",
      "[INFO][tpe] tpe_transform took 0.001848 seconds\n",
      "[INFO][tpe] TPE using 26/26 trials with best loss 10.382119\n",
      "[INFO][tpe] tpe_transform took 0.001669 seconds\n",
      "[INFO][tpe] TPE using 27/27 trials with best loss 10.382119\n",
      "[INFO][tpe] tpe_transform took 0.001765 seconds\n",
      "[INFO][tpe] TPE using 28/28 trials with best loss 10.382119\n",
      "[INFO][tpe] tpe_transform took 0.001764 seconds\n",
      "[INFO][tpe] TPE using 29/29 trials with best loss 10.382119\n",
      "[INFO][tpe] tpe_transform took 0.001780 seconds\n",
      "[INFO][tpe] TPE using 30/30 trials with best loss 10.382119\n",
      "[INFO][tpe] tpe_transform took 0.002074 seconds\n",
      "[INFO][tpe] TPE using 31/31 trials with best loss 10.382119\n",
      "[INFO][tpe] tpe_transform took 0.001803 seconds\n",
      "[INFO][tpe] TPE using 32/32 trials with best loss 10.382119\n",
      "[INFO][tpe] tpe_transform took 0.001734 seconds\n",
      "[INFO][tpe] TPE using 33/33 trials with best loss 10.382119\n",
      "[INFO][tpe] tpe_transform took 0.001800 seconds\n",
      "[INFO][tpe] TPE using 34/34 trials with best loss 10.382119\n",
      "[INFO][tpe] tpe_transform took 0.001836 seconds\n",
      "[INFO][tpe] TPE using 35/35 trials with best loss 10.382119\n",
      "[INFO][tpe] tpe_transform took 0.001724 seconds\n",
      "[INFO][tpe] TPE using 36/36 trials with best loss 10.382119\n",
      "[INFO][tpe] tpe_transform took 0.001804 seconds\n",
      "[INFO][tpe] TPE using 37/37 trials with best loss 10.382119\n",
      "[INFO][tpe] tpe_transform took 0.001752 seconds\n",
      "[INFO][tpe] TPE using 38/38 trials with best loss 10.382119\n",
      "[INFO][tpe] tpe_transform took 0.001678 seconds\n",
      "[INFO][tpe] TPE using 39/39 trials with best loss 10.382119\n",
      "[INFO][tpe] tpe_transform took 0.001790 seconds\n",
      "[INFO][tpe] TPE using 40/40 trials with best loss 10.382119\n",
      "[INFO][tpe] tpe_transform took 0.001832 seconds\n",
      "[INFO][tpe] TPE using 41/41 trials with best loss 10.382119\n",
      "[INFO][tpe] tpe_transform took 0.001818 seconds\n",
      "[INFO][tpe] TPE using 42/42 trials with best loss 10.382119\n",
      "[INFO][tpe] tpe_transform took 0.001788 seconds\n",
      "[INFO][tpe] TPE using 43/43 trials with best loss 10.382119\n",
      "[INFO][tpe] tpe_transform took 0.001764 seconds\n",
      "[INFO][tpe] TPE using 44/44 trials with best loss 10.382119\n",
      "[INFO][tpe] tpe_transform took 0.001711 seconds\n",
      "[INFO][tpe] TPE using 45/45 trials with best loss 10.382119\n",
      "[INFO][tpe] tpe_transform took 0.001828 seconds\n",
      "[INFO][tpe] TPE using 46/46 trials with best loss 10.382119\n",
      "[INFO][tpe] tpe_transform took 0.001788 seconds\n",
      "[INFO][tpe] TPE using 47/47 trials with best loss 10.382119\n",
      "[INFO][tpe] tpe_transform took 0.001845 seconds\n",
      "[INFO][tpe] TPE using 48/48 trials with best loss 10.382119\n",
      "[INFO][tpe] tpe_transform took 0.001727 seconds\n",
      "[INFO][tpe] TPE using 49/49 trials with best loss 10.382119\n",
      "[INFO][tpe] tpe_transform took 0.001763 seconds\n",
      "[INFO][tpe] TPE using 50/50 trials with best loss 10.382119\n",
      "[INFO][tpe] tpe_transform took 0.001803 seconds\n",
      "[INFO][tpe] TPE using 51/51 trials with best loss 10.382119\n",
      "[INFO][tpe] tpe_transform took 0.001805 seconds\n",
      "[INFO][tpe] TPE using 52/52 trials with best loss 10.382119\n",
      "[INFO][tpe] tpe_transform took 0.001683 seconds\n",
      "[INFO][tpe] TPE using 53/53 trials with best loss 10.382119\n",
      "[INFO][tpe] tpe_transform took 0.001719 seconds\n",
      "[INFO][tpe] TPE using 54/54 trials with best loss 10.382119\n",
      "[INFO][tpe] tpe_transform took 0.001838 seconds\n",
      "[INFO][tpe] TPE using 55/55 trials with best loss 10.382119\n",
      "[INFO][tpe] tpe_transform took 0.001788 seconds\n",
      "[INFO][tpe] TPE using 56/56 trials with best loss 10.382119\n",
      "[INFO][tpe] tpe_transform took 0.002022 seconds\n",
      "[INFO][tpe] TPE using 57/57 trials with best loss 10.382119\n",
      "[INFO][tpe] tpe_transform took 0.001693 seconds\n",
      "[INFO][tpe] TPE using 58/58 trials with best loss 10.382119\n",
      "[INFO][tpe] tpe_transform took 0.001822 seconds\n",
      "[INFO][tpe] TPE using 59/59 trials with best loss 10.382119\n",
      "[INFO][tpe] tpe_transform took 0.001750 seconds\n",
      "[INFO][tpe] TPE using 60/60 trials with best loss 10.382119\n",
      "[INFO][tpe] tpe_transform took 0.001788 seconds\n",
      "[INFO][tpe] TPE using 61/61 trials with best loss 10.379887\n",
      "[INFO][tpe] tpe_transform took 0.001687 seconds\n",
      "[INFO][tpe] TPE using 62/62 trials with best loss 10.379887\n",
      "[INFO][tpe] tpe_transform took 0.001811 seconds\n",
      "[INFO][tpe] TPE using 63/63 trials with best loss 10.379887\n",
      "[INFO][tpe] tpe_transform took 0.001761 seconds\n",
      "[INFO][tpe] TPE using 64/64 trials with best loss 10.379887\n",
      "[INFO][tpe] tpe_transform took 0.001925 seconds\n",
      "[INFO][tpe] TPE using 65/65 trials with best loss 10.379887\n",
      "[INFO][tpe] tpe_transform took 0.001847 seconds\n",
      "[INFO][tpe] TPE using 66/66 trials with best loss 10.379887\n",
      "[INFO][tpe] tpe_transform took 0.001772 seconds\n",
      "[INFO][tpe] TPE using 67/67 trials with best loss 10.375119\n",
      "[INFO][tpe] tpe_transform took 0.001798 seconds\n",
      "[INFO][tpe] TPE using 68/68 trials with best loss 10.375119\n",
      "[INFO][tpe] tpe_transform took 0.001688 seconds\n",
      "[INFO][tpe] TPE using 69/69 trials with best loss 10.375119\n",
      "[INFO][tpe] tpe_transform took 0.001817 seconds\n",
      "[INFO][tpe] TPE using 70/70 trials with best loss 10.375119\n",
      "[INFO][tpe] tpe_transform took 0.001791 seconds\n",
      "[INFO][tpe] TPE using 71/71 trials with best loss 10.375119\n",
      "[INFO][tpe] tpe_transform took 0.001688 seconds\n",
      "[INFO][tpe] TPE using 72/72 trials with best loss 10.375119\n",
      "[INFO][tpe] tpe_transform took 0.001759 seconds\n",
      "[INFO][tpe] TPE using 73/73 trials with best loss 10.375119\n",
      "[INFO][tpe] tpe_transform took 0.001852 seconds\n",
      "[INFO][tpe] TPE using 74/74 trials with best loss 10.375119\n",
      "[INFO][tpe] tpe_transform took 0.001764 seconds\n",
      "[INFO][tpe] TPE using 75/75 trials with best loss 10.375119\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO][tpe] tpe_transform took 0.001796 seconds\n",
      "[INFO][tpe] TPE using 76/76 trials with best loss 10.375119\n",
      "[INFO][tpe] tpe_transform took 0.001768 seconds\n",
      "[INFO][tpe] TPE using 77/77 trials with best loss 10.375119\n",
      "[INFO][tpe] tpe_transform took 0.001690 seconds\n",
      "[INFO][tpe] TPE using 78/78 trials with best loss 10.375119\n",
      "[INFO][tpe] tpe_transform took 0.001829 seconds\n",
      "[INFO][tpe] TPE using 79/79 trials with best loss 10.375119\n",
      "[INFO][tpe] tpe_transform took 0.001692 seconds\n",
      "[INFO][tpe] TPE using 80/80 trials with best loss 10.375119\n",
      "[INFO][tpe] tpe_transform took 0.001773 seconds\n",
      "[INFO][tpe] TPE using 81/81 trials with best loss 10.375119\n",
      "[INFO][tpe] tpe_transform took 0.001772 seconds\n",
      "[INFO][tpe] TPE using 82/82 trials with best loss 10.375119\n",
      "[INFO][tpe] tpe_transform took 0.001686 seconds\n",
      "[INFO][tpe] TPE using 83/83 trials with best loss 10.375119\n",
      "[INFO][tpe] tpe_transform took 0.001788 seconds\n",
      "[INFO][tpe] TPE using 84/84 trials with best loss 10.375119\n",
      "[INFO][tpe] tpe_transform took 0.001781 seconds\n",
      "[INFO][tpe] TPE using 85/85 trials with best loss 10.375119\n",
      "[INFO][tpe] tpe_transform took 0.001847 seconds\n",
      "[INFO][tpe] TPE using 86/86 trials with best loss 10.375119\n",
      "[INFO][tpe] tpe_transform took 0.001856 seconds\n",
      "[INFO][tpe] TPE using 87/87 trials with best loss 10.375119\n",
      "[INFO][tpe] tpe_transform took 0.001702 seconds\n",
      "[INFO][tpe] TPE using 88/88 trials with best loss 10.375119\n",
      "[INFO][tpe] tpe_transform took 0.001881 seconds\n",
      "[INFO][tpe] TPE using 89/89 trials with best loss 10.375119\n",
      "[INFO][tpe] tpe_transform took 0.001767 seconds\n",
      "[INFO][tpe] TPE using 90/90 trials with best loss 10.375119\n",
      "[INFO][tpe] tpe_transform took 0.001788 seconds\n",
      "[INFO][tpe] TPE using 91/91 trials with best loss 10.375119\n",
      "[INFO][tpe] tpe_transform took 0.001780 seconds\n",
      "[INFO][tpe] TPE using 92/92 trials with best loss 10.375119\n",
      "[INFO][tpe] tpe_transform took 0.001722 seconds\n",
      "[INFO][tpe] TPE using 93/93 trials with best loss 10.375119\n",
      "[INFO][tpe] tpe_transform took 0.001805 seconds\n",
      "[INFO][tpe] TPE using 94/94 trials with best loss 10.375119\n",
      "[INFO][tpe] tpe_transform took 0.001719 seconds\n",
      "[INFO][tpe] TPE using 95/95 trials with best loss 10.375119\n",
      "[INFO][tpe] tpe_transform took 0.001757 seconds\n",
      "[INFO][tpe] TPE using 96/96 trials with best loss 10.375119\n",
      "[INFO][tpe] tpe_transform took 0.001769 seconds\n",
      "[INFO][tpe] TPE using 97/97 trials with best loss 10.375119\n",
      "[INFO][tpe] tpe_transform took 0.001846 seconds\n",
      "[INFO][tpe] TPE using 98/98 trials with best loss 10.375119\n",
      "[INFO][tpe] tpe_transform took 0.001668 seconds\n",
      "[INFO][tpe] TPE using 99/99 trials with best loss 10.375119\n",
      "[INFO][tpe] tpe_transform took 0.001844 seconds\n",
      "[INFO][tpe] TPE using 100/100 trials with best loss 10.375119\n",
      "[INFO][tpe] tpe_transform took 0.001826 seconds\n",
      "[INFO][tpe] TPE using 101/101 trials with best loss 10.375119\n",
      "[INFO][tpe] tpe_transform took 0.001826 seconds\n",
      "[INFO][tpe] TPE using 102/102 trials with best loss 10.375119\n",
      "[INFO][tpe] tpe_transform took 0.001776 seconds\n",
      "[INFO][tpe] TPE using 103/103 trials with best loss 10.375119\n",
      "[INFO][tpe] tpe_transform took 0.001770 seconds\n",
      "[INFO][tpe] TPE using 104/104 trials with best loss 10.375119\n",
      "[INFO][tpe] tpe_transform took 0.001743 seconds\n",
      "[INFO][tpe] TPE using 105/105 trials with best loss 10.375119\n",
      "[INFO][tpe] tpe_transform took 0.001921 seconds\n",
      "[INFO][tpe] TPE using 106/106 trials with best loss 10.375119\n",
      "[INFO][tpe] tpe_transform took 0.001818 seconds\n",
      "[INFO][tpe] TPE using 107/107 trials with best loss 10.375119\n",
      "[INFO][tpe] tpe_transform took 0.001781 seconds\n",
      "[INFO][tpe] TPE using 108/108 trials with best loss 10.375119\n",
      "[INFO][tpe] tpe_transform took 0.001779 seconds\n",
      "[INFO][tpe] TPE using 109/109 trials with best loss 10.375119\n",
      "[INFO][tpe] tpe_transform took 0.001698 seconds\n",
      "[INFO][tpe] TPE using 110/110 trials with best loss 10.375119\n",
      "[INFO][tpe] tpe_transform took 0.001683 seconds\n",
      "[INFO][tpe] TPE using 111/111 trials with best loss 10.375119\n",
      "[INFO][tpe] tpe_transform took 0.001710 seconds\n",
      "[INFO][tpe] TPE using 112/112 trials with best loss 10.375119\n",
      "[INFO][tpe] tpe_transform took 0.002021 seconds\n",
      "[INFO][tpe] TPE using 113/113 trials with best loss 10.375119\n",
      "[INFO][tpe] tpe_transform took 0.002158 seconds\n",
      "[INFO][tpe] TPE using 114/114 trials with best loss 10.375119\n",
      "[INFO][tpe] tpe_transform took 0.001815 seconds\n",
      "[INFO][tpe] TPE using 115/115 trials with best loss 10.375119\n",
      "[INFO][tpe] tpe_transform took 0.001829 seconds\n",
      "[INFO][tpe] TPE using 116/116 trials with best loss 10.375119\n",
      "[INFO][tpe] tpe_transform took 0.001817 seconds\n",
      "[INFO][tpe] TPE using 117/117 trials with best loss 10.375119\n",
      "[INFO][tpe] tpe_transform took 0.001784 seconds\n",
      "[INFO][tpe] TPE using 118/118 trials with best loss 10.375119\n",
      "[INFO][tpe] tpe_transform took 0.001679 seconds\n",
      "[INFO][tpe] TPE using 119/119 trials with best loss 10.375119\n",
      "[INFO][tpe] tpe_transform took 0.001657 seconds\n",
      "[INFO][tpe] TPE using 120/120 trials with best loss 10.375119\n",
      "[INFO][tpe] tpe_transform took 0.001894 seconds\n",
      "[INFO][tpe] TPE using 121/121 trials with best loss 10.375119\n",
      "[INFO][tpe] tpe_transform took 0.001770 seconds\n",
      "[INFO][tpe] TPE using 122/122 trials with best loss 10.375119\n",
      "[INFO][tpe] tpe_transform took 0.001869 seconds\n",
      "[INFO][tpe] TPE using 123/123 trials with best loss 10.375119\n",
      "[INFO][tpe] tpe_transform took 0.001829 seconds\n",
      "[INFO][tpe] TPE using 124/124 trials with best loss 10.375119\n",
      "[INFO][tpe] tpe_transform took 0.001719 seconds\n",
      "[INFO][tpe] TPE using 125/125 trials with best loss 10.375119\n",
      "[INFO][tpe] tpe_transform took 0.001820 seconds\n",
      "[INFO][tpe] TPE using 126/126 trials with best loss 10.375119\n",
      "[INFO][tpe] tpe_transform took 0.001674 seconds\n",
      "[INFO][tpe] TPE using 127/127 trials with best loss 10.375119\n",
      "[INFO][tpe] tpe_transform took 0.001764 seconds\n",
      "[INFO][tpe] TPE using 128/128 trials with best loss 10.375119\n",
      "[INFO][tpe] tpe_transform took 0.001750 seconds\n",
      "[INFO][tpe] TPE using 129/129 trials with best loss 10.375119\n",
      "[INFO][tpe] tpe_transform took 0.001762 seconds\n",
      "[INFO][tpe] TPE using 130/130 trials with best loss 10.375119\n",
      "[INFO][tpe] tpe_transform took 0.001754 seconds\n",
      "[INFO][tpe] TPE using 131/131 trials with best loss 10.375119\n",
      "[INFO][tpe] tpe_transform took 0.001717 seconds\n",
      "[INFO][tpe] TPE using 132/132 trials with best loss 10.375119\n",
      "[INFO][tpe] tpe_transform took 0.001847 seconds\n",
      "[INFO][tpe] TPE using 133/133 trials with best loss 10.375119\n",
      "[INFO][tpe] tpe_transform took 0.001913 seconds\n",
      "[INFO][tpe] TPE using 134/134 trials with best loss 10.375119\n",
      "[INFO][tpe] tpe_transform took 0.001978 seconds\n",
      "[INFO][tpe] TPE using 135/135 trials with best loss 10.375119\n",
      "[INFO][tpe] tpe_transform took 0.001883 seconds\n",
      "[INFO][tpe] TPE using 136/136 trials with best loss 10.375119\n",
      "[INFO][tpe] tpe_transform took 0.002200 seconds\n",
      "[INFO][tpe] TPE using 137/137 trials with best loss 10.375119\n",
      "[INFO][tpe] tpe_transform took 0.001788 seconds\n",
      "[INFO][tpe] TPE using 138/138 trials with best loss 10.375119\n",
      "[INFO][tpe] tpe_transform took 0.001706 seconds\n",
      "[INFO][tpe] TPE using 139/139 trials with best loss 10.375119\n",
      "[INFO][tpe] tpe_transform took 0.001764 seconds\n",
      "[INFO][tpe] TPE using 140/140 trials with best loss 10.375119\n",
      "[INFO][tpe] tpe_transform took 0.001797 seconds\n",
      "[INFO][tpe] TPE using 141/141 trials with best loss 10.375119\n",
      "[INFO][tpe] tpe_transform took 0.001781 seconds\n",
      "[INFO][tpe] TPE using 142/142 trials with best loss 10.375119\n",
      "[INFO][tpe] tpe_transform took 0.001812 seconds\n",
      "[INFO][tpe] TPE using 143/143 trials with best loss 10.375119\n",
      "[INFO][tpe] tpe_transform took 0.002159 seconds\n",
      "[INFO][tpe] TPE using 144/144 trials with best loss 10.375119\n",
      "[INFO][tpe] tpe_transform took 0.001699 seconds\n",
      "[INFO][tpe] TPE using 145/145 trials with best loss 10.375119\n",
      "[INFO][tpe] tpe_transform took 0.001685 seconds\n",
      "[INFO][tpe] TPE using 146/146 trials with best loss 10.375119\n",
      "[INFO][tpe] tpe_transform took 0.001810 seconds\n",
      "[INFO][tpe] TPE using 147/147 trials with best loss 10.375119\n",
      "[INFO][tpe] tpe_transform took 0.001806 seconds\n",
      "[INFO][tpe] TPE using 148/148 trials with best loss 10.375119\n",
      "[INFO][tpe] tpe_transform took 0.001813 seconds\n",
      "[INFO][tpe] TPE using 149/149 trials with best loss 10.375119\n",
      "[INFO][tpe] tpe_transform took 0.001796 seconds\n",
      "[INFO][tpe] TPE using 150/150 trials with best loss 10.375119\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO][tpe] tpe_transform took 0.001780 seconds\n",
      "[INFO][tpe] TPE using 151/151 trials with best loss 10.373855\n",
      "[INFO][tpe] tpe_transform took 0.001767 seconds\n",
      "[INFO][tpe] TPE using 152/152 trials with best loss 10.373855\n",
      "[INFO][tpe] tpe_transform took 0.001822 seconds\n",
      "[INFO][tpe] TPE using 153/153 trials with best loss 10.373855\n",
      "[INFO][tpe] tpe_transform took 0.001827 seconds\n",
      "[INFO][tpe] TPE using 154/154 trials with best loss 10.373855\n",
      "[INFO][tpe] tpe_transform took 0.001731 seconds\n",
      "[INFO][tpe] TPE using 155/155 trials with best loss 10.373855\n",
      "[INFO][tpe] tpe_transform took 0.001767 seconds\n",
      "[INFO][tpe] TPE using 156/156 trials with best loss 10.373855\n",
      "[INFO][tpe] tpe_transform took 0.001772 seconds\n",
      "[INFO][tpe] TPE using 157/157 trials with best loss 10.373855\n",
      "[INFO][tpe] tpe_transform took 0.001794 seconds\n",
      "[INFO][tpe] TPE using 158/158 trials with best loss 10.373855\n",
      "[INFO][tpe] tpe_transform took 0.001791 seconds\n",
      "[INFO][tpe] TPE using 159/159 trials with best loss 10.373855\n",
      "[INFO][tpe] tpe_transform took 0.001817 seconds\n",
      "[INFO][tpe] TPE using 160/160 trials with best loss 10.373855\n",
      "[INFO][tpe] tpe_transform took 0.001810 seconds\n",
      "[INFO][tpe] TPE using 161/161 trials with best loss 10.373855\n",
      "[INFO][tpe] tpe_transform took 0.001822 seconds\n",
      "[INFO][tpe] TPE using 162/162 trials with best loss 10.373855\n",
      "[INFO][tpe] tpe_transform took 0.001779 seconds\n",
      "[INFO][tpe] TPE using 163/163 trials with best loss 10.373855\n",
      "[INFO][tpe] tpe_transform took 0.001865 seconds\n",
      "[INFO][tpe] TPE using 164/164 trials with best loss 10.373855\n",
      "[INFO][tpe] tpe_transform took 0.001731 seconds\n",
      "[INFO][tpe] TPE using 165/165 trials with best loss 10.373855\n",
      "[INFO][tpe] tpe_transform took 0.001655 seconds\n",
      "[INFO][tpe] TPE using 166/166 trials with best loss 10.373855\n",
      "[INFO][tpe] tpe_transform took 0.001858 seconds\n",
      "[INFO][tpe] TPE using 167/167 trials with best loss 10.373855\n",
      "[INFO][tpe] tpe_transform took 0.002365 seconds\n",
      "[INFO][tpe] TPE using 168/168 trials with best loss 10.373855\n",
      "[INFO][tpe] tpe_transform took 0.001793 seconds\n",
      "[INFO][tpe] TPE using 169/169 trials with best loss 10.373855\n",
      "[INFO][tpe] tpe_transform took 0.001769 seconds\n",
      "[INFO][tpe] TPE using 170/170 trials with best loss 10.373855\n",
      "[INFO][tpe] tpe_transform took 0.001777 seconds\n",
      "[INFO][tpe] TPE using 171/171 trials with best loss 10.373855\n",
      "[INFO][tpe] tpe_transform took 0.001704 seconds\n",
      "[INFO][tpe] TPE using 172/172 trials with best loss 10.373855\n",
      "[INFO][tpe] tpe_transform took 0.001687 seconds\n",
      "[INFO][tpe] TPE using 173/173 trials with best loss 10.373855\n",
      "[INFO][tpe] tpe_transform took 0.001655 seconds\n",
      "[INFO][tpe] TPE using 174/174 trials with best loss 10.373855\n",
      "[INFO][tpe] tpe_transform took 0.001796 seconds\n",
      "[INFO][tpe] TPE using 175/175 trials with best loss 10.373855\n",
      "[INFO][tpe] tpe_transform took 0.001792 seconds\n",
      "[INFO][tpe] TPE using 176/176 trials with best loss 10.373855\n",
      "[INFO][tpe] tpe_transform took 0.001907 seconds\n",
      "[INFO][tpe] TPE using 177/177 trials with best loss 10.373855\n",
      "[INFO][tpe] tpe_transform took 0.001810 seconds\n",
      "[INFO][tpe] TPE using 178/178 trials with best loss 10.373855\n",
      "[INFO][tpe] tpe_transform took 0.001797 seconds\n",
      "[INFO][tpe] TPE using 179/179 trials with best loss 10.373855\n",
      "[INFO][tpe] tpe_transform took 0.001795 seconds\n",
      "[INFO][tpe] TPE using 180/180 trials with best loss 10.373855\n",
      "[INFO][tpe] tpe_transform took 0.001691 seconds\n",
      "[INFO][tpe] TPE using 181/181 trials with best loss 10.373855\n",
      "[INFO][tpe] tpe_transform took 0.001914 seconds\n",
      "[INFO][tpe] TPE using 182/182 trials with best loss 10.373855\n",
      "[INFO][tpe] tpe_transform took 0.001792 seconds\n",
      "[INFO][tpe] TPE using 183/183 trials with best loss 10.373855\n",
      "[INFO][tpe] tpe_transform took 0.002084 seconds\n",
      "[INFO][tpe] TPE using 184/184 trials with best loss 10.373855\n",
      "[INFO][tpe] tpe_transform took 0.001774 seconds\n",
      "[INFO][tpe] TPE using 185/185 trials with best loss 10.373855\n",
      "[INFO][tpe] tpe_transform took 0.001655 seconds\n",
      "[INFO][tpe] TPE using 186/186 trials with best loss 10.373855\n",
      "[INFO][tpe] tpe_transform took 0.001765 seconds\n",
      "[INFO][tpe] TPE using 187/187 trials with best loss 10.373855\n",
      "[INFO][tpe] tpe_transform took 0.001885 seconds\n",
      "[INFO][tpe] TPE using 188/188 trials with best loss 10.373855\n",
      "[INFO][tpe] tpe_transform took 0.001772 seconds\n",
      "[INFO][tpe] TPE using 189/189 trials with best loss 10.373855\n",
      "[INFO][tpe] tpe_transform took 0.001783 seconds\n",
      "[INFO][tpe] TPE using 190/190 trials with best loss 10.373855\n",
      "[INFO][tpe] tpe_transform took 0.001795 seconds\n",
      "[INFO][tpe] TPE using 191/191 trials with best loss 10.373855\n",
      "[INFO][tpe] tpe_transform took 0.001688 seconds\n",
      "[INFO][tpe] TPE using 192/192 trials with best loss 10.373855\n",
      "[INFO][tpe] tpe_transform took 0.001673 seconds\n",
      "[INFO][tpe] TPE using 193/193 trials with best loss 10.373855\n",
      "[INFO][tpe] tpe_transform took 0.001791 seconds\n",
      "[INFO][tpe] TPE using 194/194 trials with best loss 10.373855\n",
      "[INFO][tpe] tpe_transform took 0.001905 seconds\n",
      "[INFO][tpe] TPE using 195/195 trials with best loss 10.373855\n",
      "[INFO][tpe] tpe_transform took 0.001797 seconds\n",
      "[INFO][tpe] TPE using 196/196 trials with best loss 10.373855\n",
      "[INFO][tpe] tpe_transform took 0.001781 seconds\n",
      "[INFO][tpe] TPE using 197/197 trials with best loss 10.373855\n",
      "[INFO][tpe] tpe_transform took 0.001807 seconds\n",
      "[INFO][tpe] TPE using 198/198 trials with best loss 10.373855\n",
      "[INFO][tpe] tpe_transform took 0.001731 seconds\n",
      "[INFO][tpe] TPE using 199/199 trials with best loss 10.373855\n",
      "[INFO][model] caching spread model to /home/morelandjs/.local/share/nflmodel/spread.pkl\n",
      "[INFO][model] calibrating total hyperparameters\n",
      "[INFO][data] updating season 2019 week 17\n",
      "[INFO][tpe] tpe_transform took 0.001551 seconds\n",
      "[INFO][tpe] TPE using 0 trials\n",
      "[INFO][tpe] tpe_transform took 0.001815 seconds\n",
      "[INFO][tpe] TPE using 1/1 trials with best loss 10.862792\n",
      "[INFO][tpe] tpe_transform took 0.001834 seconds\n",
      "[INFO][tpe] TPE using 2/2 trials with best loss 10.849026\n",
      "[INFO][tpe] tpe_transform took 0.001828 seconds\n",
      "[INFO][tpe] TPE using 3/3 trials with best loss 10.799525\n",
      "[INFO][tpe] tpe_transform took 0.001676 seconds\n",
      "[INFO][tpe] TPE using 4/4 trials with best loss 10.799525\n",
      "[INFO][tpe] tpe_transform took 0.001837 seconds\n",
      "[INFO][tpe] TPE using 5/5 trials with best loss 10.799525\n",
      "[INFO][tpe] tpe_transform took 0.001790 seconds\n",
      "[INFO][tpe] TPE using 6/6 trials with best loss 10.739733\n",
      "[INFO][tpe] tpe_transform took 0.001787 seconds\n",
      "[INFO][tpe] TPE using 7/7 trials with best loss 10.739733\n",
      "[INFO][tpe] tpe_transform took 0.001771 seconds\n",
      "[INFO][tpe] TPE using 8/8 trials with best loss 10.739733\n",
      "[INFO][tpe] tpe_transform took 0.002223 seconds\n",
      "[INFO][tpe] TPE using 9/9 trials with best loss 10.739733\n",
      "[INFO][tpe] tpe_transform took 0.001687 seconds\n",
      "[INFO][tpe] TPE using 10/10 trials with best loss 10.739733\n",
      "[INFO][tpe] tpe_transform took 0.001764 seconds\n",
      "[INFO][tpe] TPE using 11/11 trials with best loss 10.739733\n",
      "[INFO][tpe] tpe_transform took 0.001719 seconds\n",
      "[INFO][tpe] TPE using 12/12 trials with best loss 10.739733\n",
      "[INFO][tpe] tpe_transform took 0.001780 seconds\n",
      "[INFO][tpe] TPE using 13/13 trials with best loss 10.739733\n",
      "[INFO][tpe] tpe_transform took 0.001809 seconds\n",
      "[INFO][tpe] TPE using 14/14 trials with best loss 10.739733\n",
      "[INFO][tpe] tpe_transform took 0.001791 seconds\n",
      "[INFO][tpe] TPE using 15/15 trials with best loss 10.706435\n",
      "[INFO][tpe] tpe_transform took 0.001805 seconds\n",
      "[INFO][tpe] TPE using 16/16 trials with best loss 10.706435\n",
      "[INFO][tpe] tpe_transform took 0.001808 seconds\n",
      "[INFO][tpe] TPE using 17/17 trials with best loss 10.706435\n",
      "[INFO][tpe] tpe_transform took 0.001824 seconds\n",
      "[INFO][tpe] TPE using 18/18 trials with best loss 10.706435\n",
      "[INFO][tpe] tpe_transform took 0.001829 seconds\n",
      "[INFO][tpe] TPE using 19/19 trials with best loss 10.706435\n",
      "[INFO][tpe] tpe_transform took 0.001684 seconds\n",
      "[INFO][tpe] TPE using 20/20 trials with best loss 10.706435\n",
      "[INFO][tpe] tpe_transform took 0.001759 seconds\n",
      "[INFO][tpe] TPE using 21/21 trials with best loss 10.706435\n",
      "[INFO][tpe] tpe_transform took 0.001811 seconds\n",
      "[INFO][tpe] TPE using 22/22 trials with best loss 10.706435\n",
      "[INFO][tpe] tpe_transform took 0.001802 seconds\n",
      "[INFO][tpe] TPE using 23/23 trials with best loss 10.706435\n",
      "[INFO][tpe] tpe_transform took 0.001785 seconds\n",
      "[INFO][tpe] TPE using 24/24 trials with best loss 10.706435\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO][tpe] tpe_transform took 0.001788 seconds\n",
      "[INFO][tpe] TPE using 25/25 trials with best loss 10.706435\n",
      "[INFO][tpe] tpe_transform took 0.001695 seconds\n",
      "[INFO][tpe] TPE using 26/26 trials with best loss 10.704557\n",
      "[INFO][tpe] tpe_transform took 0.001789 seconds\n",
      "[INFO][tpe] TPE using 27/27 trials with best loss 10.704557\n",
      "[INFO][tpe] tpe_transform took 0.001801 seconds\n",
      "[INFO][tpe] TPE using 28/28 trials with best loss 10.704557\n",
      "[INFO][tpe] tpe_transform took 0.001778 seconds\n",
      "[INFO][tpe] TPE using 29/29 trials with best loss 10.704557\n",
      "[INFO][tpe] tpe_transform took 0.001786 seconds\n",
      "[INFO][tpe] TPE using 30/30 trials with best loss 10.704557\n",
      "[INFO][tpe] tpe_transform took 0.001817 seconds\n",
      "[INFO][tpe] TPE using 31/31 trials with best loss 10.704557\n",
      "[INFO][tpe] tpe_transform took 0.001781 seconds\n",
      "[INFO][tpe] TPE using 32/32 trials with best loss 10.704557\n",
      "[INFO][tpe] tpe_transform took 0.001775 seconds\n",
      "[INFO][tpe] TPE using 33/33 trials with best loss 10.704557\n",
      "[INFO][tpe] tpe_transform took 0.001834 seconds\n",
      "[INFO][tpe] TPE using 34/34 trials with best loss 10.704557\n",
      "[INFO][tpe] tpe_transform took 0.001822 seconds\n",
      "[INFO][tpe] TPE using 35/35 trials with best loss 10.704557\n",
      "[INFO][tpe] tpe_transform took 0.001942 seconds\n",
      "[INFO][tpe] TPE using 36/36 trials with best loss 10.704557\n",
      "[INFO][tpe] tpe_transform took 0.001847 seconds\n",
      "[INFO][tpe] TPE using 37/37 trials with best loss 10.704557\n",
      "[INFO][tpe] tpe_transform took 0.001763 seconds\n",
      "[INFO][tpe] TPE using 38/38 trials with best loss 10.704557\n",
      "[INFO][tpe] tpe_transform took 0.001937 seconds\n",
      "[INFO][tpe] TPE using 39/39 trials with best loss 10.704557\n",
      "[INFO][tpe] tpe_transform took 0.001837 seconds\n",
      "[INFO][tpe] TPE using 40/40 trials with best loss 10.704557\n",
      "[INFO][tpe] tpe_transform took 0.001789 seconds\n",
      "[INFO][tpe] TPE using 41/41 trials with best loss 10.704557\n",
      "[INFO][tpe] tpe_transform took 0.001795 seconds\n",
      "[INFO][tpe] TPE using 42/42 trials with best loss 10.704557\n",
      "[INFO][tpe] tpe_transform took 0.001765 seconds\n",
      "[INFO][tpe] TPE using 43/43 trials with best loss 10.704557\n",
      "[INFO][tpe] tpe_transform took 0.001693 seconds\n",
      "[INFO][tpe] TPE using 44/44 trials with best loss 10.704557\n",
      "[INFO][tpe] tpe_transform took 0.001779 seconds\n",
      "[INFO][tpe] TPE using 45/45 trials with best loss 10.704557\n",
      "[INFO][tpe] tpe_transform took 0.001793 seconds\n",
      "[INFO][tpe] TPE using 46/46 trials with best loss 10.704557\n",
      "[INFO][tpe] tpe_transform took 0.001813 seconds\n",
      "[INFO][tpe] TPE using 47/47 trials with best loss 10.704557\n",
      "[INFO][tpe] tpe_transform took 0.001822 seconds\n",
      "[INFO][tpe] TPE using 48/48 trials with best loss 10.699998\n",
      "[INFO][tpe] tpe_transform took 0.001779 seconds\n",
      "[INFO][tpe] TPE using 49/49 trials with best loss 10.699998\n",
      "[INFO][tpe] tpe_transform took 0.001896 seconds\n",
      "[INFO][tpe] TPE using 50/50 trials with best loss 10.699998\n",
      "[INFO][tpe] tpe_transform took 0.001803 seconds\n",
      "[INFO][tpe] TPE using 51/51 trials with best loss 10.699998\n",
      "[INFO][tpe] tpe_transform took 0.001805 seconds\n",
      "[INFO][tpe] TPE using 52/52 trials with best loss 10.699998\n",
      "[INFO][tpe] tpe_transform took 0.001909 seconds\n",
      "[INFO][tpe] TPE using 53/53 trials with best loss 10.699998\n",
      "[INFO][tpe] tpe_transform took 0.001838 seconds\n",
      "[INFO][tpe] TPE using 54/54 trials with best loss 10.699998\n",
      "[INFO][tpe] tpe_transform took 0.001771 seconds\n",
      "[INFO][tpe] TPE using 55/55 trials with best loss 10.699998\n",
      "[INFO][tpe] tpe_transform took 0.001658 seconds\n",
      "[INFO][tpe] TPE using 56/56 trials with best loss 10.699998\n",
      "[INFO][tpe] tpe_transform took 0.001813 seconds\n",
      "[INFO][tpe] TPE using 57/57 trials with best loss 10.699998\n",
      "[INFO][tpe] tpe_transform took 0.002243 seconds\n",
      "[INFO][tpe] TPE using 58/58 trials with best loss 10.699998\n",
      "[INFO][tpe] tpe_transform took 0.001826 seconds\n",
      "[INFO][tpe] TPE using 59/59 trials with best loss 10.699998\n",
      "[INFO][tpe] tpe_transform took 0.001739 seconds\n",
      "[INFO][tpe] TPE using 60/60 trials with best loss 10.699998\n",
      "[INFO][tpe] tpe_transform took 0.001780 seconds\n",
      "[INFO][tpe] TPE using 61/61 trials with best loss 10.699998\n",
      "[INFO][tpe] tpe_transform took 0.001765 seconds\n",
      "[INFO][tpe] TPE using 62/62 trials with best loss 10.699998\n",
      "[INFO][tpe] tpe_transform took 0.001796 seconds\n",
      "[INFO][tpe] TPE using 63/63 trials with best loss 10.699998\n",
      "[INFO][tpe] tpe_transform took 0.001721 seconds\n",
      "[INFO][tpe] TPE using 64/64 trials with best loss 10.699998\n",
      "[INFO][tpe] tpe_transform took 0.001781 seconds\n",
      "[INFO][tpe] TPE using 65/65 trials with best loss 10.699998\n",
      "[INFO][tpe] tpe_transform took 0.001804 seconds\n",
      "[INFO][tpe] TPE using 66/66 trials with best loss 10.699998\n",
      "[INFO][tpe] tpe_transform took 0.001707 seconds\n",
      "[INFO][tpe] TPE using 67/67 trials with best loss 10.699998\n",
      "[INFO][tpe] tpe_transform took 0.001773 seconds\n",
      "[INFO][tpe] TPE using 68/68 trials with best loss 10.699998\n",
      "[INFO][tpe] tpe_transform took 0.001775 seconds\n",
      "[INFO][tpe] TPE using 69/69 trials with best loss 10.699998\n",
      "[INFO][tpe] tpe_transform took 0.001811 seconds\n",
      "[INFO][tpe] TPE using 70/70 trials with best loss 10.699998\n",
      "[INFO][tpe] tpe_transform took 0.001716 seconds\n",
      "[INFO][tpe] TPE using 71/71 trials with best loss 10.699998\n",
      "[INFO][tpe] tpe_transform took 0.001775 seconds\n",
      "[INFO][tpe] TPE using 72/72 trials with best loss 10.699998\n",
      "[INFO][tpe] tpe_transform took 0.001944 seconds\n",
      "[INFO][tpe] TPE using 73/73 trials with best loss 10.699998\n",
      "[INFO][tpe] tpe_transform took 0.001858 seconds\n",
      "[INFO][tpe] TPE using 74/74 trials with best loss 10.699998\n",
      "[INFO][tpe] tpe_transform took 0.001727 seconds\n",
      "[INFO][tpe] TPE using 75/75 trials with best loss 10.698735\n",
      "[INFO][tpe] tpe_transform took 0.001790 seconds\n",
      "[INFO][tpe] TPE using 76/76 trials with best loss 10.698735\n",
      "[INFO][tpe] tpe_transform took 0.001810 seconds\n",
      "[INFO][tpe] TPE using 77/77 trials with best loss 10.698735\n",
      "[INFO][tpe] tpe_transform took 0.001760 seconds\n",
      "[INFO][tpe] TPE using 78/78 trials with best loss 10.698735\n",
      "[INFO][tpe] tpe_transform took 0.001789 seconds\n",
      "[INFO][tpe] TPE using 79/79 trials with best loss 10.698735\n",
      "[INFO][tpe] tpe_transform took 0.001783 seconds\n",
      "[INFO][tpe] TPE using 80/80 trials with best loss 10.698735\n",
      "[INFO][tpe] tpe_transform took 0.001807 seconds\n",
      "[INFO][tpe] TPE using 81/81 trials with best loss 10.698735\n",
      "[INFO][tpe] tpe_transform took 0.001737 seconds\n",
      "[INFO][tpe] TPE using 82/82 trials with best loss 10.698735\n",
      "[INFO][tpe] tpe_transform took 0.001823 seconds\n",
      "[INFO][tpe] TPE using 83/83 trials with best loss 10.698735\n",
      "[INFO][tpe] tpe_transform took 0.001921 seconds\n",
      "[INFO][tpe] TPE using 84/84 trials with best loss 10.698735\n",
      "[INFO][tpe] tpe_transform took 0.001828 seconds\n",
      "[INFO][tpe] TPE using 85/85 trials with best loss 10.698735\n",
      "[INFO][tpe] tpe_transform took 0.001802 seconds\n",
      "[INFO][tpe] TPE using 86/86 trials with best loss 10.698735\n",
      "[INFO][tpe] tpe_transform took 0.002185 seconds\n",
      "[INFO][tpe] TPE using 87/87 trials with best loss 10.698735\n",
      "[INFO][tpe] tpe_transform took 0.001791 seconds\n",
      "[INFO][tpe] TPE using 88/88 trials with best loss 10.698735\n",
      "[INFO][tpe] tpe_transform took 0.001680 seconds\n",
      "[INFO][tpe] TPE using 89/89 trials with best loss 10.698735\n",
      "[INFO][tpe] tpe_transform took 0.001809 seconds\n",
      "[INFO][tpe] TPE using 90/90 trials with best loss 10.698735\n",
      "[INFO][tpe] tpe_transform took 0.001819 seconds\n",
      "[INFO][tpe] TPE using 91/91 trials with best loss 10.695714\n",
      "[INFO][tpe] tpe_transform took 0.001797 seconds\n",
      "[INFO][tpe] TPE using 92/92 trials with best loss 10.695714\n",
      "[INFO][tpe] tpe_transform took 0.001820 seconds\n",
      "[INFO][tpe] TPE using 93/93 trials with best loss 10.695714\n",
      "[INFO][tpe] tpe_transform took 0.001789 seconds\n",
      "[INFO][tpe] TPE using 94/94 trials with best loss 10.695714\n",
      "[INFO][tpe] tpe_transform took 0.001754 seconds\n",
      "[INFO][tpe] TPE using 95/95 trials with best loss 10.695714\n",
      "[INFO][tpe] tpe_transform took 0.001782 seconds\n",
      "[INFO][tpe] TPE using 96/96 trials with best loss 10.695714\n",
      "[INFO][tpe] tpe_transform took 0.001724 seconds\n",
      "[INFO][tpe] TPE using 97/97 trials with best loss 10.695714\n",
      "[INFO][tpe] tpe_transform took 0.001772 seconds\n",
      "[INFO][tpe] TPE using 98/98 trials with best loss 10.695714\n",
      "[INFO][tpe] tpe_transform took 0.001720 seconds\n",
      "[INFO][tpe] TPE using 99/99 trials with best loss 10.695714\n",
      "[INFO][tpe] tpe_transform took 0.001954 seconds\n",
      "[INFO][tpe] TPE using 100/100 trials with best loss 10.695714\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO][tpe] tpe_transform took 0.001922 seconds\n",
      "[INFO][tpe] TPE using 101/101 trials with best loss 10.695714\n",
      "[INFO][tpe] tpe_transform took 0.001804 seconds\n",
      "[INFO][tpe] TPE using 102/102 trials with best loss 10.695714\n",
      "[INFO][tpe] tpe_transform took 0.001781 seconds\n",
      "[INFO][tpe] TPE using 103/103 trials with best loss 10.694746\n",
      "[INFO][tpe] tpe_transform took 0.001863 seconds\n",
      "[INFO][tpe] TPE using 104/104 trials with best loss 10.694746\n",
      "[INFO][tpe] tpe_transform took 0.001711 seconds\n",
      "[INFO][tpe] TPE using 105/105 trials with best loss 10.694746\n",
      "[INFO][tpe] tpe_transform took 0.001805 seconds\n",
      "[INFO][tpe] TPE using 106/106 trials with best loss 10.694746\n",
      "[INFO][tpe] tpe_transform took 0.001909 seconds\n",
      "[INFO][tpe] TPE using 107/107 trials with best loss 10.694746\n",
      "[INFO][tpe] tpe_transform took 0.001810 seconds\n",
      "[INFO][tpe] TPE using 108/108 trials with best loss 10.694746\n",
      "[INFO][tpe] tpe_transform took 0.001709 seconds\n",
      "[INFO][tpe] TPE using 109/109 trials with best loss 10.694746\n",
      "[INFO][tpe] tpe_transform took 0.001697 seconds\n",
      "[INFO][tpe] TPE using 110/110 trials with best loss 10.694746\n",
      "[INFO][tpe] tpe_transform took 0.001822 seconds\n",
      "[INFO][tpe] TPE using 111/111 trials with best loss 10.694746\n",
      "[INFO][tpe] tpe_transform took 0.001834 seconds\n",
      "[INFO][tpe] TPE using 112/112 trials with best loss 10.694746\n",
      "[INFO][tpe] tpe_transform took 0.001841 seconds\n",
      "[INFO][tpe] TPE using 113/113 trials with best loss 10.694746\n",
      "[INFO][tpe] tpe_transform took 0.001797 seconds\n",
      "[INFO][tpe] TPE using 114/114 trials with best loss 10.694746\n",
      "[INFO][tpe] tpe_transform took 0.001704 seconds\n",
      "[INFO][tpe] TPE using 115/115 trials with best loss 10.694746\n",
      "[INFO][tpe] tpe_transform took 0.001856 seconds\n",
      "[INFO][tpe] TPE using 116/116 trials with best loss 10.694746\n",
      "[INFO][tpe] tpe_transform took 0.001744 seconds\n",
      "[INFO][tpe] TPE using 117/117 trials with best loss 10.694746\n",
      "[INFO][tpe] tpe_transform took 0.001796 seconds\n",
      "[INFO][tpe] TPE using 118/118 trials with best loss 10.694746\n",
      "[INFO][tpe] tpe_transform took 0.001804 seconds\n",
      "[INFO][tpe] TPE using 119/119 trials with best loss 10.694746\n",
      "[INFO][tpe] tpe_transform took 0.001936 seconds\n",
      "[INFO][tpe] TPE using 120/120 trials with best loss 10.694746\n",
      "[INFO][tpe] tpe_transform took 0.001795 seconds\n",
      "[INFO][tpe] TPE using 121/121 trials with best loss 10.694746\n",
      "[INFO][tpe] tpe_transform took 0.001785 seconds\n",
      "[INFO][tpe] TPE using 122/122 trials with best loss 10.694746\n",
      "[INFO][tpe] tpe_transform took 0.001846 seconds\n",
      "[INFO][tpe] TPE using 123/123 trials with best loss 10.694746\n",
      "[INFO][tpe] tpe_transform took 0.001702 seconds\n",
      "[INFO][tpe] TPE using 124/124 trials with best loss 10.694746\n",
      "[INFO][tpe] tpe_transform took 0.001762 seconds\n",
      "[INFO][tpe] TPE using 125/125 trials with best loss 10.694746\n",
      "[INFO][tpe] tpe_transform took 0.001837 seconds\n",
      "[INFO][tpe] TPE using 126/126 trials with best loss 10.694746\n",
      "[INFO][tpe] tpe_transform took 0.001802 seconds\n",
      "[INFO][tpe] TPE using 127/127 trials with best loss 10.694746\n",
      "[INFO][tpe] tpe_transform took 0.001799 seconds\n",
      "[INFO][tpe] TPE using 128/128 trials with best loss 10.694746\n",
      "[INFO][tpe] tpe_transform took 0.001950 seconds\n",
      "[INFO][tpe] TPE using 129/129 trials with best loss 10.694746\n",
      "[INFO][tpe] tpe_transform took 0.001788 seconds\n",
      "[INFO][tpe] TPE using 130/130 trials with best loss 10.694746\n",
      "[INFO][tpe] tpe_transform took 0.001799 seconds\n",
      "[INFO][tpe] TPE using 131/131 trials with best loss 10.694746\n",
      "[INFO][tpe] tpe_transform took 0.001731 seconds\n",
      "[INFO][tpe] TPE using 132/132 trials with best loss 10.694746\n",
      "[INFO][tpe] tpe_transform took 0.001735 seconds\n",
      "[INFO][tpe] TPE using 133/133 trials with best loss 10.694746\n",
      "[INFO][tpe] tpe_transform took 0.001824 seconds\n",
      "[INFO][tpe] TPE using 134/134 trials with best loss 10.694746\n",
      "[INFO][tpe] tpe_transform took 0.001891 seconds\n",
      "[INFO][tpe] TPE using 135/135 trials with best loss 10.694746\n",
      "[INFO][tpe] tpe_transform took 0.001672 seconds\n",
      "[INFO][tpe] TPE using 136/136 trials with best loss 10.694746\n",
      "[INFO][tpe] tpe_transform took 0.001871 seconds\n",
      "[INFO][tpe] TPE using 137/137 trials with best loss 10.694746\n",
      "[INFO][tpe] tpe_transform took 0.001851 seconds\n",
      "[INFO][tpe] TPE using 138/138 trials with best loss 10.694746\n",
      "[INFO][tpe] tpe_transform took 0.001713 seconds\n",
      "[INFO][tpe] TPE using 139/139 trials with best loss 10.694746\n",
      "[INFO][tpe] tpe_transform took 0.001801 seconds\n",
      "[INFO][tpe] TPE using 140/140 trials with best loss 10.694746\n",
      "[INFO][tpe] tpe_transform took 0.001804 seconds\n",
      "[INFO][tpe] TPE using 141/141 trials with best loss 10.694746\n",
      "[INFO][tpe] tpe_transform took 0.001841 seconds\n",
      "[INFO][tpe] TPE using 142/142 trials with best loss 10.694746\n",
      "[INFO][tpe] tpe_transform took 0.001859 seconds\n",
      "[INFO][tpe] TPE using 143/143 trials with best loss 10.694746\n",
      "[INFO][tpe] tpe_transform took 0.001903 seconds\n",
      "[INFO][tpe] TPE using 144/144 trials with best loss 10.694746\n",
      "[INFO][tpe] tpe_transform took 0.001935 seconds\n",
      "[INFO][tpe] TPE using 145/145 trials with best loss 10.694746\n",
      "[INFO][tpe] tpe_transform took 0.001800 seconds\n",
      "[INFO][tpe] TPE using 146/146 trials with best loss 10.694746\n",
      "[INFO][tpe] tpe_transform took 0.001755 seconds\n",
      "[INFO][tpe] TPE using 147/147 trials with best loss 10.694746\n",
      "[INFO][tpe] tpe_transform took 0.001853 seconds\n",
      "[INFO][tpe] TPE using 148/148 trials with best loss 10.694746\n",
      "[INFO][tpe] tpe_transform took 0.001955 seconds\n",
      "[INFO][tpe] TPE using 149/149 trials with best loss 10.694746\n",
      "[INFO][tpe] tpe_transform took 0.001712 seconds\n",
      "[INFO][tpe] TPE using 150/150 trials with best loss 10.694746\n",
      "[INFO][tpe] tpe_transform took 0.001851 seconds\n",
      "[INFO][tpe] TPE using 151/151 trials with best loss 10.694746\n",
      "[INFO][tpe] tpe_transform took 0.001852 seconds\n",
      "[INFO][tpe] TPE using 152/152 trials with best loss 10.694746\n",
      "[INFO][tpe] tpe_transform took 0.001860 seconds\n",
      "[INFO][tpe] TPE using 153/153 trials with best loss 10.694746\n",
      "[INFO][tpe] tpe_transform took 0.001846 seconds\n",
      "[INFO][tpe] TPE using 154/154 trials with best loss 10.694746\n",
      "[INFO][tpe] tpe_transform took 0.001806 seconds\n",
      "[INFO][tpe] TPE using 155/155 trials with best loss 10.694746\n",
      "[INFO][tpe] tpe_transform took 0.001873 seconds\n",
      "[INFO][tpe] TPE using 156/156 trials with best loss 10.694746\n",
      "[INFO][tpe] tpe_transform took 0.001829 seconds\n",
      "[INFO][tpe] TPE using 157/157 trials with best loss 10.694746\n",
      "[INFO][tpe] tpe_transform took 0.001706 seconds\n",
      "[INFO][tpe] TPE using 158/158 trials with best loss 10.694746\n",
      "[INFO][tpe] tpe_transform took 0.001783 seconds\n",
      "[INFO][tpe] TPE using 159/159 trials with best loss 10.694746\n",
      "[INFO][tpe] tpe_transform took 0.001673 seconds\n",
      "[INFO][tpe] TPE using 160/160 trials with best loss 10.694746\n",
      "[INFO][tpe] tpe_transform took 0.001790 seconds\n",
      "[INFO][tpe] TPE using 161/161 trials with best loss 10.694746\n",
      "[INFO][tpe] tpe_transform took 0.001853 seconds\n",
      "[INFO][tpe] TPE using 162/162 trials with best loss 10.694746\n",
      "[INFO][tpe] tpe_transform took 0.001830 seconds\n",
      "[INFO][tpe] TPE using 163/163 trials with best loss 10.694746\n",
      "[INFO][tpe] tpe_transform took 0.001736 seconds\n",
      "[INFO][tpe] TPE using 164/164 trials with best loss 10.694746\n",
      "[INFO][tpe] tpe_transform took 0.001981 seconds\n",
      "[INFO][tpe] TPE using 165/165 trials with best loss 10.694746\n",
      "[INFO][tpe] tpe_transform took 0.001804 seconds\n",
      "[INFO][tpe] TPE using 166/166 trials with best loss 10.694746\n",
      "[INFO][tpe] tpe_transform took 0.001799 seconds\n",
      "[INFO][tpe] TPE using 167/167 trials with best loss 10.694746\n",
      "[INFO][tpe] tpe_transform took 0.001724 seconds\n",
      "[INFO][tpe] TPE using 168/168 trials with best loss 10.694746\n",
      "[INFO][tpe] tpe_transform took 0.001770 seconds\n",
      "[INFO][tpe] TPE using 169/169 trials with best loss 10.694746\n",
      "[INFO][tpe] tpe_transform took 0.001786 seconds\n",
      "[INFO][tpe] TPE using 170/170 trials with best loss 10.694746\n",
      "[INFO][tpe] tpe_transform took 0.001699 seconds\n",
      "[INFO][tpe] TPE using 171/171 trials with best loss 10.694746\n",
      "[INFO][tpe] tpe_transform took 0.001809 seconds\n",
      "[INFO][tpe] TPE using 172/172 trials with best loss 10.694746\n",
      "[INFO][tpe] tpe_transform took 0.001833 seconds\n",
      "[INFO][tpe] TPE using 173/173 trials with best loss 10.694746\n",
      "[INFO][tpe] tpe_transform took 0.001805 seconds\n",
      "[INFO][tpe] TPE using 174/174 trials with best loss 10.694746\n",
      "[INFO][tpe] tpe_transform took 0.001896 seconds\n",
      "[INFO][tpe] TPE using 175/175 trials with best loss 10.694746\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO][tpe] tpe_transform took 0.001846 seconds\n",
      "[INFO][tpe] TPE using 176/176 trials with best loss 10.694746\n",
      "[INFO][tpe] tpe_transform took 0.001798 seconds\n",
      "[INFO][tpe] TPE using 177/177 trials with best loss 10.694746\n",
      "[INFO][tpe] tpe_transform took 0.001693 seconds\n",
      "[INFO][tpe] TPE using 178/178 trials with best loss 10.694746\n",
      "[INFO][tpe] tpe_transform took 0.001793 seconds\n",
      "[INFO][tpe] TPE using 179/179 trials with best loss 10.694746\n",
      "[INFO][tpe] tpe_transform took 0.001812 seconds\n",
      "[INFO][tpe] TPE using 180/180 trials with best loss 10.694746\n",
      "[INFO][tpe] tpe_transform took 0.001807 seconds\n",
      "[INFO][tpe] TPE using 181/181 trials with best loss 10.694746\n",
      "[INFO][tpe] tpe_transform took 0.001786 seconds\n",
      "[INFO][tpe] TPE using 182/182 trials with best loss 10.694746\n",
      "[INFO][tpe] tpe_transform took 0.001825 seconds\n",
      "[INFO][tpe] TPE using 183/183 trials with best loss 10.694746\n",
      "[INFO][tpe] tpe_transform took 0.001698 seconds\n",
      "[INFO][tpe] TPE using 184/184 trials with best loss 10.694746\n",
      "[INFO][tpe] tpe_transform took 0.001691 seconds\n",
      "[INFO][tpe] TPE using 185/185 trials with best loss 10.694746\n",
      "[INFO][tpe] tpe_transform took 0.001675 seconds\n",
      "[INFO][tpe] TPE using 186/186 trials with best loss 10.694746\n",
      "[INFO][tpe] tpe_transform took 0.001773 seconds\n",
      "[INFO][tpe] TPE using 187/187 trials with best loss 10.694746\n",
      "[INFO][tpe] tpe_transform took 0.001797 seconds\n",
      "[INFO][tpe] TPE using 188/188 trials with best loss 10.694746\n",
      "[INFO][tpe] tpe_transform took 0.001811 seconds\n",
      "[INFO][tpe] TPE using 189/189 trials with best loss 10.694746\n",
      "[INFO][tpe] tpe_transform took 0.001745 seconds\n",
      "[INFO][tpe] TPE using 190/190 trials with best loss 10.694746\n",
      "[INFO][tpe] tpe_transform took 0.001798 seconds\n",
      "[INFO][tpe] TPE using 191/191 trials with best loss 10.694746\n",
      "[INFO][tpe] tpe_transform took 0.001747 seconds\n",
      "[INFO][tpe] TPE using 192/192 trials with best loss 10.694746\n",
      "[INFO][tpe] tpe_transform took 0.001817 seconds\n",
      "[INFO][tpe] TPE using 193/193 trials with best loss 10.694746\n",
      "[INFO][tpe] tpe_transform took 0.001776 seconds\n",
      "[INFO][tpe] TPE using 194/194 trials with best loss 10.694746\n",
      "[INFO][tpe] tpe_transform took 0.001816 seconds\n",
      "[INFO][tpe] TPE using 195/195 trials with best loss 10.694746\n",
      "[INFO][tpe] tpe_transform took 0.001820 seconds\n",
      "[INFO][tpe] TPE using 196/196 trials with best loss 10.694746\n",
      "[INFO][tpe] tpe_transform took 0.001750 seconds\n",
      "[INFO][tpe] TPE using 197/197 trials with best loss 10.694746\n",
      "[INFO][tpe] tpe_transform took 0.001844 seconds\n",
      "[INFO][tpe] TPE using 198/198 trials with best loss 10.694746\n",
      "[INFO][tpe] tpe_transform took 0.001817 seconds\n",
      "[INFO][tpe] TPE using 199/199 trials with best loss 10.694746\n",
      "[INFO][model] caching total model to /home/morelandjs/.local/share/nflmodel/total.pkl\n"
     ]
    }
   ],
   "source": [
    "!nflmodel calibrate --steps 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the argument `--steps` (default value 100) specifies the number of calibration steps used by [hyperopt](https://github.com/hyperopt/hyperopt) to optimize the hyperparameter values. I've generally found that around 200 calibration steps is sufficient, but the convergence is already very good after only 100 steps.\n",
    "\n",
    "Note, the `MELO` model is actually very fast to train once the hyperparameter values are known, so default behavior is to retrain the model every time it is queried from the CLI. This ensures that the model predictions are always up to date."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weekly forecasts\n",
    "\n",
    "Once calibrated, the `nflmodel` package can forecast various metrics for a given NFL season and week. For instance, the command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO][nflmodel] Forecast for season 2019 week 10\n",
      "\n",
      "           favorite underdog  win prob  spread  total\n",
      "date                                                 \n",
      "2019-11-10      @NO      ATL      0.91    -9.9   47.6\n",
      "2019-11-10      BAL     @CIN      0.91    -9.2   45.7\n",
      "2019-11-10     @IND      MIA      0.71    -5.2   47.7\n",
      "2019-11-11      @SF      SEA      0.67    -4.4   48.9\n",
      "2019-11-10     @DAL      MIN      0.54    -3.0   45.0\n",
      "2019-11-10      BUF     @CLE      0.57    -2.8   42.0\n",
      "2019-11-10       KC     @TEN      0.58    -2.7   48.6\n",
      "2019-11-10     @CHI      DET      0.61    -1.8   44.7\n",
      "2019-11-07     @OAK      LAC      0.54    -1.7   43.2\n",
      "2019-11-10      @GB      CAR      0.53    -1.4   53.6\n",
      "2019-11-10     @NYJ      NYG      0.53    -1.3   46.9\n",
      "2019-11-10      ARI      @TB      0.53    -0.6   50.9\n",
      "2019-11-10       LA     @PIT      0.51    -0.3   44.9 \n",
      "\n",
      "*win probability and spread are for the favored team\n"
     ]
    }
   ],
   "source": [
    "!nflmodel forecast 2019 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "will generate predictions for the 10th week of the 2019 NFL season, using all available game data prior to the start of that week. If you do not provide the year and week, the code will try to infer the upcoming week based on the current date and known NFL season schedule.\n",
    "\n",
    "The output is structured relative to the favorite team in each matchup, with home teams indicated by an '@' symbol. For example, the forecast output above says that NO is playing at home versus ATL, where they have a 91% win probability, are favored by 9.6 points, and are predicted to combine for 47.4 total points. The games are also sorted so that the most lopsided matchups appear first and most even matchups appear last."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Team rankings\n",
    "\n",
    "The model will also provide team rankings at a given moment in time. For example, suppose I want to rank every team at precisely `datetime = 2019-11-11T20:23:06`. I can issue the command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO][nflmodel] Rankings as of 2019-11-11T20:23:06\n",
      "\n",
      "       win prob        spread         total\n",
      "rank                                       \n",
      "1     BAL  0.78  │  NE  -10.5  │   TB  50.1\n",
      "2      NE  0.77  │  BAL  -7.7  │   KC  50.1\n",
      "3     SEA  0.75  │   SF  -6.3  │  NYG  48.3\n",
      "4      SF  0.72  │  MIN  -5.8  │  BAL  48.0\n",
      "5      NO  0.71  │  HOU  -4.5  │  SEA  47.9\n",
      "6     HOU  0.69  │   LA  -4.5  │  CAR  47.6\n",
      "7     MIN  0.68  │  DAL  -4.2  │  ARI  46.8\n",
      "8      GB  0.67  │   NO  -4.1  │  ATL  46.7\n",
      "9     PIT  0.64  │  SEA  -3.2  │  OAK  46.6\n",
      "10    CAR  0.62  │  PIT  -3.0  │  PHI  46.5\n",
      "11    PHI  0.61  │   KC  -2.8  │  DET  46.4\n",
      "12     LA  0.60  │  PHI  -2.8  │   GB  46.0\n",
      "13     KC  0.57  │   GB  -2.6  │  HOU  45.9\n",
      "14    DAL  0.55  │  CAR  -2.5  │   SF  45.8\n",
      "15    OAK  0.54  │  LAC  -2.0  │  CIN  45.6\n",
      "16    CHI  0.54  │  CHI  -2.0  │   NO  45.5\n",
      "17    TEN  0.53  │  BUF  -0.7  │   LA  45.4\n",
      "18    JAX  0.51  │  JAX  -0.2  │  DAL  45.3\n",
      "19    BUF  0.50  │  DEN  -0.2  │  CLE  45.2\n",
      "20    DEN  0.48  │  CLE   1.1  │  NYJ  45.0\n",
      "21    LAC  0.48  │  ATL   1.2  │  IND  45.0\n",
      "22    CLE  0.47  │  IND   1.4  │  PIT  44.8\n",
      "23    ATL  0.43  │  TEN   1.4  │  MIA  44.8\n",
      "24    DET  0.41  │  DET   1.4  │  MIN  44.2\n",
      "25     TB  0.39  │   TB   1.7  │   NE  44.0\n",
      "26    ARI  0.38  │  OAK   3.0  │  LAC  43.4\n",
      "27    IND  0.38  │  ARI   3.1  │  JAX  43.0\n",
      "28    MIA  0.38  │  NYG   4.5  │  TEN  43.0\n",
      "29    NYJ  0.33  │  WAS   4.7  │  WAS  42.3\n",
      "30    WAS  0.33  │  CIN   4.8  │  BUF  42.2\n",
      "31    CIN  0.31  │  NYJ   5.4  │  CHI  41.7\n",
      "32    NYG  0.28  │  MIA   5.9  │  DEN  41.6 \n",
      "\n",
      "*expected performance against league average\n",
      "opponent on a neutral field\n"
     ]
    }
   ],
   "source": [
    "!nflmodel rank --datetime \"2019-11-11T20:23:06\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and see how the model thinks the teams should be ranked at that moment in time, according to their predicted performance against a league average opponent on a neutral field. The far left column is each team's predicted win probability, the middle column is its predicted Vegas point spread, and the far right column is its predicted point total.\n",
    "\n",
    "The table above, for instance, tells me that BAL is most likely to win a generic matchup, while NE is most likely to blow out their opponent, and TB is the most likely to find itself in a shoot out.\n",
    "\n",
    "## Individual game predictions\n",
    "\n",
    "One attractive feature of the `MELO` base estimator is that it predicts full probability _distributions_. This enables the model to do all sorts of cool things like predict interquartile ranges for the point spread or draw samples of a matchup's point total distribution.\n",
    "\n",
    "Most notably, it means that the model can estimate the probability that the point spread (or point total) falls above or below a given line. This means that it can evaluate the profitability of various bets point spread and point total lines.\n",
    "\n",
    "This capability is accessed using the `nflmodel predict` entry point. For example, suppose you want to analyze the game `2019-12-01 CLE at MIA` with the following betting profile\n",
    "\n",
    "|     | SPREAD      | TOTAL         |\n",
    "|-----|-------------|---------------|\n",
    "| CLE | -9.5 (-110) | O 46.5 (-105) |\n",
    "| MIA | +9.5 (-110) | U 46.5 (-115) |\n",
    "\n",
    "This is accomplished by calling the `nfl predict` runner with the following arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO][nflmodel] 2019-12-01T00:00:00 CLE at MIA\n",
      "\n",
      "                away   home\n",
      "team             CLE    MIA\n",
      "win prob         68%    32%\n",
      "spread         -12.6   12.6\n",
      "total           44.2   44.2\n",
      "score             28     16\n",
      "spread cover     59%    41%\n",
      "spread return    14%   -24%\n",
      "                           \n",
      "                over  under\n",
      "total cover      45%    55%\n",
      "total return    -13%     4% \n",
      "\n",
      "*actual return rate lower than predicted\n"
     ]
    }
   ],
   "source": [
    "!nflmodel predict \"2019-12-01\" CLE MIA --spread -110 -110 9.5 --total -105 -115 46.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a lot of information here, so let's unpack what it means. First, the model believes CLE is a heavy favorite on the road. CLE's predicted win probability is 68%, their predicted point spread is -12.6 points, and their predicted point total is 44.2 points (according to the model). This would correspond to a characteristic final score of 28-16 CLE over MIA with fairly large uncertainties.\n",
    "\n",
    "The model is also providing input on the Vegas point spread and point total lines. It expects CLE to cover their point spread line 59% of the time which is good for a 14% ROI, accounting for the house cut. Conversely, betting on MIA is expected to net a loss of 24% on average.\n",
    "\n",
    "The over/under metrics are reported in a similar fashion. The model thinks there's a 55% chance the total score goes under the published Vegas line which is good for a 4% return on average. Similarly, taking the over is expected to net a 13% loss.\n",
    "\n",
    "#### Quarterback changes\n",
    "\n",
    "While it has not been readily apparent up to this point, the model accounts for changes at the quarterback position. It does this by tracking ratings at both the team and quarterback level.\n",
    "\n",
    "For example, suppose Tom Brady were the only quarterback that ever played for the Patriots. Then there would be two associated ratings, one for T.Brady and one for NE, which are effectively identical. If however, Tom Brady left NE and went to play for SF for the last two years of his career, his rating would diverge from NE's rating and begin to track the performance of SF.\n",
    "\n",
    "I take the weighted average of QB level and team level ratings when generating the effective rating for each upcoming game. In this way, I mix together the historical performance of the team with its QB. The weighted average is controlled by the `qb_weight` hyperparameter which is fixed when calibrating the model.\n",
    "\n",
    "The model has no direct knowledge of QB injuries, so you'll have to explicitly tell the model to generate predictions with a different quarterback if that's what you intend to do. At the moment, the only runner that can accommodate QB injuries is the `nflmodel predict` runner.\n",
    "\n",
    "Suppose, for example, you want to see how CLE would perform on the road against PHI on 2019-12-01 if Carson Wentz went out with an injury practicing before the game. First let's see how the two teams would matchup if Wentz never got hurt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO][nflmodel] 2019-12-01T00:00:00 CLE-B.Mayfield at PHI-C.Wentz\n",
      "\n",
      "                    away         home\n",
      "team      CLE-B.Mayfield  PHI-C.Wentz\n",
      "win prob             45%          55%\n",
      "spread               4.4         -4.4\n",
      "total               44.3         44.3\n",
      "score                 20           24 \n",
      "\n",
      "*actual return rate lower than predicted\n"
     ]
    }
   ],
   "source": [
    "!nflmodel predict \"2019-12-01\" CLE-B.Mayfield PHI-C.Wentz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the predictions are exactly the same if I omit the quarterback suffixes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO][nflmodel] 2019-12-01T00:00:00 CLE at PHI\n",
      "\n",
      "          away  home\n",
      "team       CLE   PHI\n",
      "win prob   45%   55%\n",
      "spread     4.4  -4.4\n",
      "total     44.3  44.3\n",
      "score       20    24 \n",
      "\n",
      "*actual return rate lower than predicted\n"
     ]
    }
   ],
   "source": [
    "!nflmodel predict \"2019-12-01\" CLE PHI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is because both Baker Mayfield and Carson Wentz played in the game preceding the specified date, so the model assumes they are the starting QBs for upcoming games by default.\n",
    "\n",
    "Suppose now, that Wentz got hurt. We can specify his backup QB Josh McCown to see how that would affect the model predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO][nflmodel] 2019-12-01T00:00:00 CLE at PHI-J.McCown\n",
      "\n",
      "          away          home\n",
      "team       CLE  PHI-J.McCown\n",
      "win prob   53%           47%\n",
      "spread    -2.8           2.8\n",
      "total     44.3          44.3\n",
      "score       24            21 \n",
      "\n",
      "*actual return rate lower than predicted\n"
     ]
    }
   ],
   "source": [
    "!nflmodel predict \"2019-12-01\" CLE PHI-J.McCown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This creates roughly a 7 point swing in the point spread and CLE is now the favorite. In faireness to Josh McCown, the magnitude of this point shift is not purely a statement about the better QB. It also accounts for the fact that PHI has not been game planning for McCown, and the offense is not built around him.\n",
    "\n",
    "## Validation\n",
    "\n",
    "The `nflmodel` package includes a command line runner to validate the predictions of the model. If the model predictions are statistically robust, then the distribution of standardized residuals will be unit normal and the distribution of residual quantiles will be uniform.\n",
    "\n",
    "This is a very powerful test of the model's veracity, but it does not necessarily mean the model is accurate. Rather it tests whether the model is correctly reporting its own uncertainties. To quantify the model's accuracy, `nflmodel validate` also reports the models mean absolute prediction error for seasons 2011 through present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO][validate] spread residual mean: 0.09\n",
      "[INFO][validate] spread residual mean absolute error: 10.36\n",
      "[INFO][validate] total residual mean: 0.10\n",
      "[INFO][validate] total residual mean absolute error: 10.70\n"
     ]
    }
   ],
   "source": [
    "!nflmodel validate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will produce two figures, `validate_spread.png` and `validate_total.png` in the current working directory. For example, the point spread validation figure is shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![spread validation](validate_spread.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model's point spreads have a mean absolute error of 10.36 points and its point totals have a mean absolute error of 10.70 points. The table below compares the model's point spread mean absolute error to Vegas for the specific season range 2011–2019.\n",
    "\n",
    "| MAE   | SPREAD    | TOTAL     |\n",
    "|-------|-----------|-----------|\n",
    "| MODEL | 10.36 pts | 10.70 pts |\n",
    "| VEGAS | 10.23 pts | 10.49 pts |\n",
    "\n",
    "The model is less accurate than Vegas, but these numbers are still promising considering that many factors remain missing from the model such as weather and personnel changes.\n",
    "\n",
    "## Betting simulation\n",
    "\n",
    "I've also bundled a small script `nflmodel/tutorial/simulate-bets.py` inside the tutorial directory which can simulate the performance of the model as a betting tool on historical games. Hereafter, I restrict my attention to point spreads since I'm currently neglecting many factors which are specifically important to point total values like stadium type and weather.\n",
    "\n",
    "The general idea is as follows. I backtest the model and estimate the probability that the home team and away team each cover their Vegas spread (scraped from covers.com) at the moment just before kickoff. If the model believes that either team will cover their spread with a likelihood greater than X, where X is a fixed decision threshold, then I place a simulated bet on that team.\n",
    "\n",
    "When the threshold X=50%, the simulation places bets on every game, and when the threshold X ≫ 50%, the simulation is far more selective with the games it chooses to bet on. If I set the decision threshold X > 90% then the model cannot find any games with sufficiently high confidence to bet on.\n",
    "\n",
    "In principle, one expects the model to get more bets correct when X is large because it is more confident in those bets. The goal here is to see if there is a threshold X which is sufficiently large to yield a positive ROI.\n",
    "\n",
    "Technically, I need more information to compute the model's ROI than just the historical spread. I also need the vigorish or \"juice\" on each spread which is the cut taken by the house in order to place a bet. Typically this ranges anywhere from -100 to -120 for spread bets, which means you might need to risk up to 120 dollars in order to win 100 dollars. More extreme vigs exist but they are uncommon.\n",
    "\n",
    "Unfortunately, **I do not have the historical vigs for these lines**, precluding a true ROI simulation. However, you can rest assured that the ROI will be strongly negative if the model is not getting more games right than wrong. So for now, let's just see if I can demonstrate that the model is better than 50-50 _with statistical significance_.\n",
    "\n",
    "Statistical significance here is key. As I increase the betting threshold, I reduce my validation statistics because the number of qualifying games drops. For large values of the threshold X, the model may only find a dozen or so qualifying games to bet on. This means the results of the simulation will be noisy and it will be possible to be duped by statistical fluctuations (we've all seen someone flip four heads in a row).\n",
    "\n",
    "To calculate statistical significance, I compute the 90% interquartile range for the null model, i.e. the range of reasonable outcomes for independent random wins sampled with 50% probability. In other words, I compare my model's success rate to what one would expect from random chance.\n",
    "\n",
    "The results of this calculation are shown below for a decision threshold X=0.75."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO][simulate-bets] 40 won, 29 lost\r\n",
      "[INFO][simulate-bets] 0.58% correct model; 0.42-0.58% random chance\r\n",
      "[INFO][simulate-bets] Vegas mean abs error: 11.46 pts\r\n",
      "[INFO][simulate-bets] Model mean abs error: 10.26 pts\r\n"
     ]
    }
   ],
   "source": [
    "!python3 simulate-bets.py 0.75"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that the model is performing at the 90% quantile level, i.e. not something you'd use to bet a ton of money, but I think impressive nonetheless."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Who's going to win Super Bowl LIV?\n",
    "\n",
    "Unfortunately, I have not prepped the model to work on the post-season yet. There's nothing that precludes applying the model to the post-season, it just involves some more work, and I haven't had time to do it yet.\n",
    "\n",
    "In any event, I think this tutorial shows that the Vegas lines are quite accurate, and it is non-trivial to build a model that beats them. The small number of available games makes NFL modeling a uniquely interesting problem, and I've learned quite a bit on my own quest to build a better model.\n",
    "\n",
    "Please feel free to contact me at morelandjs@gmail.com with questions and comments.\n",
    "\n",
    "Thank you!\n",
    "\n",
    "-Scott"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
